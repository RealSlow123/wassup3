{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6002dcc9-7527-4970-96b5-7cbfce2ca693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyinstaller\n",
      "  Downloading pyinstaller-6.8.0-py3-none-win_amd64.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: setuptools>=42.0.0 in c:\\users\\gkdme\\anaconda3\\lib\\site-packages (from pyinstaller) (68.2.2)\n",
      "Collecting altgraph (from pyinstaller)\n",
      "  Downloading altgraph-0.17.4-py2.py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting pyinstaller-hooks-contrib>=2024.6 (from pyinstaller)\n",
      "  Downloading pyinstaller_hooks_contrib-2024.7-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: packaging>=22.0 in c:\\users\\gkdme\\anaconda3\\lib\\site-packages (from pyinstaller) (23.1)\n",
      "Collecting pefile>=2022.5.30 (from pyinstaller)\n",
      "  Downloading pefile-2023.2.7-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting pywin32-ctypes>=0.2.1 (from pyinstaller)\n",
      "  Downloading pywin32_ctypes-0.2.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "Downloading pyinstaller-6.8.0-py3-none-win_amd64.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------  1.3/1.3 MB 41.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 27.4 MB/s eta 0:00:00\n",
      "Downloading pefile-2023.2.7-py3-none-any.whl (71 kB)\n",
      "   ---------------------------------------- 0.0/71.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 71.8/71.8 kB 3.8 MB/s eta 0:00:00\n",
      "Downloading pyinstaller_hooks_contrib-2024.7-py2.py3-none-any.whl (341 kB)\n",
      "   ---------------------------------------- 0.0/341.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 341.3/341.3 kB ? eta 0:00:00\n",
      "Downloading pywin32_ctypes-0.2.2-py3-none-any.whl (30 kB)\n",
      "Downloading altgraph-0.17.4-py2.py3-none-any.whl (21 kB)\n",
      "Installing collected packages: altgraph, pywin32-ctypes, pyinstaller-hooks-contrib, pefile, pyinstaller\n",
      "  Attempting uninstall: pywin32-ctypes\n",
      "    Found existing installation: pywin32-ctypes 0.2.0\n",
      "    Uninstalling pywin32-ctypes-0.2.0:\n",
      "      Successfully uninstalled pywin32-ctypes-0.2.0\n",
      "Successfully installed altgraph-0.17.4 pefile-2023.2.7 pyinstaller-6.8.0 pyinstaller-hooks-contrib-2024.7 pywin32-ctypes-0.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyinstaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206958e5-0d78-4798-ab4f-8cf8c25edf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import math, time, os, pandas as pd, urllib.request\n",
    "\n",
    "options = Options()\n",
    "options.add_argument('--window-size=974,1047')\n",
    "options.add_argument('--window-position=-7,0')\n",
    "options.add_experimental_option(\"detach\", True)\n",
    "\n",
    "search = input('검색어:')\n",
    "cnt = int(input('스크래핑 할 건수는 몇건입니까?: '))\n",
    "page_cnt = math.ceil(cnt / 10)  # 크롤링 할 전체 페이지 수 \n",
    "\n",
    "now = time.localtime()\n",
    "date_format = '%04d%02d%02d'%(now.tm_year, now.tm_mon, now.tm_mday)\n",
    "f_dir = f'{os.getcwd()}\\\\{search}여행기사_{cnt}건_{date_format}'\n",
    "os.makedirs(f_dir)\n",
    "\n",
    "URL = 'https://korean.visitkorea.or.kr/search/search_list.do?keyword='+search\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.get(URL)\n",
    "time.sleep(2)\n",
    "# 여행기사 더보기 클릭\n",
    "driver.find_element(By.CSS_SELECTOR, \"#s_recommend > .more_view > a\").click()\n",
    "time.sleep(2)\n",
    "\n",
    "title_list = []\n",
    "contents_list = []\n",
    "img_url_list = []\n",
    "\n",
    "def page_work():\n",
    "    result = driver.find_elements(By.CSS_SELECTOR,'#search_result .tit>a')\n",
    "    global contents_no, cnt\n",
    "    global title_list, contents_list, img_url_list\n",
    "\n",
    "    for item in result:\n",
    "        contents_no += 1\n",
    "\n",
    "        if contents_no <= cnt :    \n",
    "            print(f'[콘텐츠 {contents_no}]')  \n",
    "            item.send_keys(Keys.ENTER) # .click()은 에러 잘남\n",
    "\n",
    "            time.sleep(2)\n",
    "\n",
    "            # 이미지 추출을 위해 미리 스크롤\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "\n",
    "            html = driver.page_source\n",
    "            html_dom = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "            title = html_dom.find(id='topTitle')\n",
    "            title_list.append(title.text)\n",
    "            print(title.text)\n",
    "\n",
    "            img_tag_list = html_dom.select('.img_typeBox img')\n",
    "            img_url_list = [item['src'] for item in img_tag_list]\n",
    "\n",
    "            contents = driver.find_elements(By.CLASS_NAME, 'txt_p')\n",
    "            contents_merge = ' '.join([item.text for item in contents])        \n",
    "            contents_list.append(contents_merge)           \n",
    "\n",
    "            driver.back()\n",
    "            time.sleep(2)     \n",
    "\n",
    "\n",
    "def file_export():\n",
    "\n",
    "    DF = pd.DataFrame({\"제목\":title_list, \"내용\":contents_list})\n",
    "    filename = f'{search}여행기사_{cnt}건_{date_format}.xlsx'\n",
    "    DF.to_excel(f_dir+'\\\\'+filename)\n",
    "    print(f'====== {page_no} 페이지 {filename} 파일 저장 완료 ======')\n",
    "\n",
    "\n",
    "    no = 0\n",
    "    for src in img_url_list:\n",
    "        # 다운로드  (주소, 파일이름)\n",
    "        urllib.request.urlretrieve(src, f'{f_dir}\\\\{page_no}_{no}.jpg')\n",
    "        no += 1\n",
    "    print(f'====== {page_no} 페이지 {f_dir} 디렉토리 이미지 저장 완료 ======')\n",
    "\n",
    "\n",
    "contents_no = 0\n",
    "today = time.localtime()\n",
    "print('스크래핑 프로그램 실행')\n",
    "\n",
    "for page_no in range(1, page_cnt+1):    \n",
    "    print(f'====== {page_no} 페이지 스크래핑 시작 ======')\n",
    "    page_work()\n",
    "    print(f'====== {page_no} 페이지 스크래핑 작업중 ======')\n",
    "    file_export()\n",
    "    print(f'====== {page_no} 페이지 스크래핑 완료 ======')\n",
    "    if page_no < page_cnt:\n",
    "        driver.find_element(By.XPATH, f'/html/body/div[3]/div/div[1]/div[14]/a[{page_no+1}]').click()\n",
    "        time.sleep(2)\n",
    "\n",
    "print('스크래핑 프로그램 종료')\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0e4ddc2-c797-4054-8be7-f26d6eeb603c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "검색어: 제주도\n",
      "스크래핑 할 건수는 몇건입니까?:  10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "스크래핑 프로그램 실행\n",
      "====== 1 페이지 스크래핑 시작 ======\n",
      "[콘텐츠 1]\n",
      "해변산책부터 레이싱까지, 제주 반려동물 동반여행 추천 코스\n",
      "[콘텐츠 2]\n"
     ]
    },
    {
     "ename": "StaleElementReferenceException",
     "evalue": "Message: stale element reference: stale element not found\n  (Session info: chrome=123.0.6312.58); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#stale-element-reference-exception\nStacktrace:\n\tGetHandleVerifier [0x00007FF68BD97032+63090]\n\t(No symbol) [0x00007FF68BD02C82]\n\t(No symbol) [0x00007FF68BB9EC65]\n\t(No symbol) [0x00007FF68BBAE78B]\n\t(No symbol) [0x00007FF68BBAEFEA]\n\t(No symbol) [0x00007FF68BBA4B29]\n\t(No symbol) [0x00007FF68BBA2D3A]\n\t(No symbol) [0x00007FF68BBA6159]\n\t(No symbol) [0x00007FF68BBA6200]\n\t(No symbol) [0x00007FF68BBE5664]\n\t(No symbol) [0x00007FF68BBE5752]\n\t(No symbol) [0x00007FF68BBDD152]\n\t(No symbol) [0x00007FF68BC06FDA]\n\t(No symbol) [0x00007FF68BBDA00A]\n\t(No symbol) [0x00007FF68BC071F0]\n\t(No symbol) [0x00007FF68BC23412]\n\t(No symbol) [0x00007FF68BC06D83]\n\t(No symbol) [0x00007FF68BBD83A8]\n\t(No symbol) [0x00007FF68BBD9441]\n\tGetHandleVerifier [0x00007FF68C1925AD+4238317]\n\tGetHandleVerifier [0x00007FF68C1CF70D+4488525]\n\tGetHandleVerifier [0x00007FF68C1C79EF+4456495]\n\tGetHandleVerifier [0x00007FF68BE70576+953270]\n\t(No symbol) [0x00007FF68BD0E54F]\n\t(No symbol) [0x00007FF68BD09224]\n\t(No symbol) [0x00007FF68BD0935B]\n\t(No symbol) [0x00007FF68BCF9B94]\n\tBaseThreadInitThunk [0x00007FFC996E257D+29]\n\tRtlUserThreadStart [0x00007FFC9B12AF28+40]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStaleElementReferenceException\u001b[0m            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 92\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page_no \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, page_cnt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):    \n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m====== \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage_no\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 페이지 스크래핑 시작 ======\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 92\u001b[0m     page_work()\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m====== \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage_no\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 페이지 스크래핑 작업중 ======\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     94\u001b[0m     file_export()\n",
      "Cell \u001b[1;32mIn[22], line 45\u001b[0m, in \u001b[0;36mpage_work\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[콘텐츠 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontents_no\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m#import pdb;pdb.set_trace()\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m             item\u001b[38;5;241m.\u001b[39msend_keys(Keys\u001b[38;5;241m.\u001b[39mENTER) \u001b[38;5;66;03m# .click()은 에러 잘남\u001b[39;00m\n\u001b[0;32m     47\u001b[0m             time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     49\u001b[0m             \u001b[38;5;66;03m# 이미지 추출을 위해 미리 스크롤\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py:231\u001b[0m, in \u001b[0;36mWebElement.send_keys\u001b[1;34m(self, *value)\u001b[0m\n\u001b[0;32m    228\u001b[0m             remote_files\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upload(file))\n\u001b[0;32m    229\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(remote_files)\n\u001b[1;32m--> 231\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute(\n\u001b[0;32m    232\u001b[0m     Command\u001b[38;5;241m.\u001b[39mSEND_KEYS_TO_ELEMENT, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(keys_to_typing(value)), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: keys_to_typing(value)}\n\u001b[0;32m    233\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py:395\u001b[0m, in \u001b[0;36mWebElement._execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    393\u001b[0m     params \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    394\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent\u001b[38;5;241m.\u001b[39mexecute(command, params)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:354\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    352\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 354\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n\u001b[0;32m    355\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mStaleElementReferenceException\u001b[0m: Message: stale element reference: stale element not found\n  (Session info: chrome=123.0.6312.58); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#stale-element-reference-exception\nStacktrace:\n\tGetHandleVerifier [0x00007FF68BD97032+63090]\n\t(No symbol) [0x00007FF68BD02C82]\n\t(No symbol) [0x00007FF68BB9EC65]\n\t(No symbol) [0x00007FF68BBAE78B]\n\t(No symbol) [0x00007FF68BBAEFEA]\n\t(No symbol) [0x00007FF68BBA4B29]\n\t(No symbol) [0x00007FF68BBA2D3A]\n\t(No symbol) [0x00007FF68BBA6159]\n\t(No symbol) [0x00007FF68BBA6200]\n\t(No symbol) [0x00007FF68BBE5664]\n\t(No symbol) [0x00007FF68BBE5752]\n\t(No symbol) [0x00007FF68BBDD152]\n\t(No symbol) [0x00007FF68BC06FDA]\n\t(No symbol) [0x00007FF68BBDA00A]\n\t(No symbol) [0x00007FF68BC071F0]\n\t(No symbol) [0x00007FF68BC23412]\n\t(No symbol) [0x00007FF68BC06D83]\n\t(No symbol) [0x00007FF68BBD83A8]\n\t(No symbol) [0x00007FF68BBD9441]\n\tGetHandleVerifier [0x00007FF68C1925AD+4238317]\n\tGetHandleVerifier [0x00007FF68C1CF70D+4488525]\n\tGetHandleVerifier [0x00007FF68C1C79EF+4456495]\n\tGetHandleVerifier [0x00007FF68BE70576+953270]\n\t(No symbol) [0x00007FF68BD0E54F]\n\t(No symbol) [0x00007FF68BD09224]\n\t(No symbol) [0x00007FF68BD0935B]\n\t(No symbol) [0x00007FF68BCF9B94]\n\tBaseThreadInitThunk [0x00007FFC996E257D+29]\n\tRtlUserThreadStart [0x00007FFC9B12AF28+40]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import math, time, os, pandas as pd, urllib.request\n",
    "\n",
    "options = Options()\n",
    "options.add_argument('--window-size=974,1047')\n",
    "options.add_argument('--window-position=-7,0')\n",
    "options.add_experimental_option(\"detach\", True)\n",
    "\n",
    "search = input('검색어:')\n",
    "cnt = int(input('스크래핑 할 건수는 몇건입니까?: '))\n",
    "page_cnt = math.ceil(cnt / 10)  # 크롤링 할 전체 페이지 수 \n",
    "\n",
    "now = time.localtime()\n",
    "date_format = '%04d%02d%02d'%(now.tm_year, now.tm_mon, now.tm_mday)\n",
    "f_dir = f'{os.getcwd()}\\\\{search}여행기사_{cnt}건_{date_format}'\n",
    "os.makedirs(f_dir)\n",
    "\n",
    "URL = 'https://korean.visitkorea.or.kr/search/search_list.do?keyword='+search\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.get(URL)\n",
    "time.sleep(2)\n",
    "# 여행기사 더보기 클릭\n",
    "driver.find_element(By.CSS_SELECTOR, \"#s_recommend > .more_view > a\").click()\n",
    "time.sleep(2)\n",
    "\n",
    "title_list = []\n",
    "contents_list = []\n",
    "img_url_list = []\n",
    "\n",
    "def page_work():\n",
    "    result = driver.find_elements(By.CSS_SELECTOR,'#search_result .tit>a')\n",
    "    global contents_no, cnt\n",
    "    global title_list, contents_list, img_url_list\n",
    "\n",
    "    for item in result:\n",
    "        contents_no += 1\n",
    "\n",
    "        if contents_no <= cnt :    \n",
    "            print(f'[콘텐츠 {contents_no}]')\n",
    "#import pdb;pdb.set_trace()\n",
    "            item.send_keys(Keys.ENTER) # .click()은 에러 잘남\n",
    "            \n",
    "            time.sleep(2)\n",
    "\n",
    "            # 이미지 추출을 위해 미리 스크롤\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "\n",
    "            html = driver.page_source\n",
    "            html_dom = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "            title = html_dom.find(id='topTitle')\n",
    "            title_list.append(title.text)\n",
    "            print(title.text)\n",
    "\n",
    "            img_tag_list = html_dom.select('.img_typeBox img')\n",
    "            img_url_list = [item['src'] for item in img_tag_list]\n",
    "\n",
    "            contents = driver.find_elements(By.CLASS_NAME, 'txt_p')\n",
    "            contents_merge = ' '.join([item.text for item in contents])        \n",
    "            contents_list.append(contents_merge)           \n",
    "\n",
    "            driver.back()\n",
    "            time.sleep(2)     \n",
    "\n",
    "\n",
    "def file_export():\n",
    "\n",
    "    DF = pd.DataFrame({\"제목\":title_list, \"내용\":contents_list})\n",
    "    filename = f'{search}여행기사_{cnt}건_{date_format}.xlsx'\n",
    "    DF.to_excel(f_dir+'\\\\'+filename)\n",
    "    print(f'====== {page_no} 페이지 {filename} 파일 저장 완료 ======')\n",
    "\n",
    "\n",
    "    no = 0\n",
    "    for src in img_url_list:\n",
    "        # 다운로드  (주소, 파일이름)\n",
    "        urllib.request.urlretrieve(src, f'{f_dir}\\\\{page_no}_{no}.jpg')\n",
    "        no += 1\n",
    "    print(f'====== {page_no} 페이지 {f_dir} 디렉토리 이미지 저장 완료 ======')\n",
    "\n",
    "\n",
    "contents_no = 0\n",
    "today = time.localtime()\n",
    "print('스크래핑 프로그램 실행')\n",
    "\n",
    "for page_no in range(1, page_cnt+1):    \n",
    "    print(f'====== {page_no} 페이지 스크래핑 시작 ======')\n",
    "    page_work()\n",
    "    print(f'====== {page_no} 페이지 스크래핑 작업중 ======')\n",
    "    file_export()\n",
    "    print(f'====== {page_no} 페이지 스크래핑 완료 ======')\n",
    "    if page_no < page_cnt:\n",
    "        driver.find_element(By.XPATH, f'/html/body/div[3]/div/div[1]/div[14]/a[{page_no+1}]').click()\n",
    "        time.sleep(2)\n",
    "\n",
    "print('스크래핑 프로그램 종료')\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "053ded7d-7a78-45c9-abd6-d226ab154e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "검색어: 제주도\n",
      "스크래핑 할 건수는 몇건입니까?:  111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "스크래핑 프로그램 실행\n",
      "====== 1 페이지 스크래핑 시작 ======\n",
      "[1] 해변산책부터 레이싱까지, 제주 반려동물 동반여행 추천 코스\n",
      "[2] 제주도가 품은 신비의 화산 숲, 환상숲곶자왈공원\n",
      "[3] 한국관광공사 추천 2월 걷기여행길, 이른 봄을 맞이하는 제주도 걷기길\n",
      "[4] 제주도의 숨겨진 재미를 찾는\n",
      "<놀멍, 배우멍 제주마을체험 - 1일형 체험학습>\n",
      "[5] 제주도 조천읍 버킷리스트 여행지 작성하기\n",
      "[6] 김창열의 회귀 철학을 건축으로 표현한 미술관, 제주도립김창열미술관\n",
      "[7] [국내 트레킹 추천]\n",
      "제주 올레길 걷기 여행,\n",
      "준비부터 코스 선택까지 꿀팁 총정리!\n",
      "[8] 4월 제주도 가볼 만한, 가파도 청보리 축제\n",
      "[9] 제주도 유채꽃 명소 추천, 봄에 떠나는 인생샷 여행\n",
      "[10] 소소한 '제주여행' 꿀팁! 〈겨울편〉\n",
      "====== 1 페이지 콘텐츠 저장 중 ======\n",
      "====== 콘텐츠 10개, 제주도여행기사_111건_20240627.csv 파일 저장 완료 ======\n",
      "====== 이미지 15개 저장 완료 ======\n",
      "====== 1 페이지 스크래핑 완료 ======\n",
      "====== 2 페이지 스크래핑 시작 ======\n",
      "[11] 연말연시 제주도 여행 추천 코스,\n",
      "가족여행 가볼 만한 곳\n",
      "[12] 새하얀 눈으로 덮인 제주도 설경 명소 4\n",
      "[13] 제주 6월 걷기 좋은 길, 성안올레 1코스 원도심 투어\n",
      "[14] 사계절 자연의 아름다움을 간직한 제주시 댕댕이 추천코스\n",
      "[15] 제주도 '한정'여행지,\n",
      "제주도의 이색 공간을 찾아서\n",
      "[16] 멈춤과 완보로 만나는 제주 생각하는정원\n",
      "[17] 2월 놓치지 말아야 할 제주 관광 10선\n",
      "<2월 제주, 먼저 온 봄기운에 마음 돌랑돌랑>\n",
      "[18] 제주의 아크로폴리스를 아시나요? - 제주 원도심 (제주목관아 중심)\n",
      "[19] 맛있는 음식에 여행의 기쁨은 두 배! <제주 식도락 여행>\n",
      "[20] 제주 여행, 은빛 물결이 일렁이는 제주도 억새 명소 모음\n",
      "====== 2 페이지 콘텐츠 저장 중 ======\n",
      "====== 콘텐츠 20개, 제주도여행기사_111건_20240627.csv 파일 저장 완료 ======\n",
      "====== 이미지 16개 저장 완료 ======\n",
      "====== 2 페이지 스크래핑 완료 ======\n",
      "====== 3 페이지 스크래핑 시작 ======\n",
      "[21] 반려견과 떠나는 여행 <제주 반려견 숙소&카페>\n",
      "[22] 설렘과 여유가 가득한 제주 여행\n",
      "[23] 빛으로 물든 '로맨틱 제주' <제주 야간 관광지>\n",
      "[24] 12월 놓치지 말아야 할 제주 관광 10선\n",
      "<올해도 애쓴 당신과 나, 12월의 제주에서 쉬멍쉬멍>\n",
      "[25] 마음까지 화사해지는\n",
      "제주도 봄꽃 여행\n",
      "[26] 제주도 서쪽에서 즐기는 꽉 찬 여행 코스\n",
      "[27] [팜스테이션] 제주공항과 아름다운 해안이 인접한 워케이션 공간\n",
      "[28] 제주의 숨겨진 구석구석을 살펴보자, 제주도 박물관 여행\n",
      "[29] 안심관광에 알맞은 최고의 야외 공연과 체험 - 제주 더마(馬)파크\n",
      "[30] [질그랭이 거점 센터] 조용한 제주 해안마을에서 즐기는 워케이션\n",
      "====== 3 페이지 콘텐츠 저장 중 ======\n",
      "====== 콘텐츠 30개, 제주도여행기사_111건_20240627.csv 파일 저장 완료 ======\n",
      "====== 이미지 8개 저장 완료 ======\n",
      "====== 3 페이지 스크래핑 완료 ======\n",
      "====== 4 페이지 스크래핑 시작 ======\n",
      "[31] [제주] 푸른 밤의 따뜻한 꿈\n",
      "[32] 1월 놓치지 말아야 할 제주 관광 10선\n",
      "<멋진 새날을 희망하며, 엄블랑한 1월 제주>\n",
      "[33] 섬 속의 섬 그리고 가장 제주다운 섬, 제주 우도\n",
      "[34] 제주 해녀들의 “호오이 호오이” 숨비소리 찾는 여행 – 제주해녀박물관과 숨비소리길\n",
      "[35] 제주도 서귀포 가볼 만한 곳,\n",
      "초록이 가득한 남쪽 여행 코스\n",
      "[36] 돌이 나에게 말을 걸어오는 시적(詩的)인 곳– 제주돌문화공원\n",
      "[37] [디어먼데이 제주] 월요병이 없는 세상을 꿈꾸는 이들의 제주 워케이션\n",
      "[38] 비움과 채움의 조화로 완성한 웰니스 라이프, 제주901\n",
      "[39] 제주의 자연과 제주인의 삶을 최단시간에 알 수 있는 곳! - 제주민속자연사박물관\n",
      "[40] 환경생태가 살아있는 진짜 제주의 숲을 만나다 – 제주절물자연휴양림과 장생의 숲길\n",
      "====== 4 페이지 콘텐츠 저장 중 ======\n",
      "====== 콘텐츠 40개, 제주도여행기사_111건_20240627.csv 파일 저장 완료 ======\n",
      "====== 이미지 15개 저장 완료 ======\n",
      "====== 4 페이지 스크래핑 완료 ======\n",
      "====== 5 페이지 스크래핑 시작 ======\n",
      "[41] [제주도 가볼 만한 곳] 섬 안에 섬을 여행하는 방법, 가파도 여행 코스\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 106\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page_no \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, page_cnt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):    \n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m====== \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage_no\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 페이지 스크래핑 시작 ======\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 106\u001b[0m     page_work()\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m====== \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage_no\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 페이지 콘텐츠 저장 중 ======\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    108\u001b[0m     file_export()\n",
      "Cell \u001b[1;32mIn[1], line 55\u001b[0m, in \u001b[0;36mpage_work\u001b[1;34m()\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# 이미지 추출을 위해 미리 스크롤\u001b[39;00m\n\u001b[0;32m     54\u001b[0m driver\u001b[38;5;241m.\u001b[39mexecute_script(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwindow.scrollTo(0, document.body.scrollHeight)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 55\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# 상세페이지 소스 추출\u001b[39;00m\n\u001b[0;32m     58\u001b[0m html \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mpage_source\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import urllib, math, time, os, pandas as pd\n",
    "\n",
    "options = Options()\n",
    "options.add_argument('--window-size=974,1047')\n",
    "options.add_argument('--window-position=953,0')\n",
    "options.add_experimental_option(\"detach\", True)\n",
    "\n",
    "search = input('검색어:')\n",
    "goal_cnt = int(input('스크래핑 할 건수는 몇건입니까?: '))\n",
    "page_cnt = math.ceil(goal_cnt / 10)  # 크롤링 할 전체 페이지 수 \n",
    "\n",
    "now = time.localtime()\n",
    "date_format = '%04d%02d%02d'%(now.tm_year, now.tm_mon, now.tm_mday)\n",
    "f_dir = f'{os.getcwd()}\\\\output\\\\{search}여행기사_{goal_cnt}건_{date_format}'\n",
    "os.makedirs(f_dir, exist_ok=True) # 디렉토리가 미리 존재해도 에러나지 않도록\n",
    "\n",
    "URL = 'https://korean.visitkorea.or.kr/search/search_list.do?keyword='+search\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.get(URL)\n",
    "time.sleep(2)\n",
    "\n",
    "# 여행기사 더보기 클릭\n",
    "driver.find_element(By.CSS_SELECTOR, \"#s_recommend > .more_view > a\").click()\n",
    "time.sleep(2)\n",
    "\n",
    "contents_no = 1\n",
    "title_list = []\n",
    "contents_list = []\n",
    "img_url_list = []\n",
    "\n",
    "def page_work():\n",
    "    global contents_no, goal_cnt, title_list, contents_list, img_url_list\n",
    "    result = driver.find_elements(By.CSS_SELECTOR,'#search_result .tit>a')\n",
    "    \n",
    "    for i in range(0, len(result)):\n",
    "        if contents_no <= goal_cnt :    \n",
    "            \n",
    "            # 페이지 변경으로인한 DOM객체 변경 문제로 소스 다시 추출하기\n",
    "            result = driver.find_elements(By.CSS_SELECTOR,'#search_result .tit>a')\n",
    "            title = result[i].text\n",
    "            title_list.append(title)\n",
    "            \n",
    "            print(f'[{contents_no}] {title}') \n",
    "            \n",
    "            result[i].send_keys(Keys.ENTER) # .click()은 에러 잘남    \n",
    "            time.sleep(2)\n",
    "            \n",
    "            # 이미지 추출을 위해 미리 스크롤\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # 상세페이지 소스 추출\n",
    "            html = driver.page_source\n",
    "            html_dom = BeautifulSoup(html, 'lxml')\n",
    "          \n",
    "            img_tag_list = html_dom.select('.img_typeBox img')\n",
    "            img_url_list = [item['src'] for item in img_tag_list]\n",
    "\n",
    "            contents = driver.find_elements(By.CLASS_NAME, 'txt_p')\n",
    "            contents_merge = ' '.join([item.text for item in contents])        \n",
    "            contents_list.append(contents_merge)           \n",
    "            \n",
    "            driver.back()\n",
    "            time.sleep(2)     \n",
    "            contents_no += 1\n",
    "            \n",
    "        if contents_no % 50 == 0: # 5page 단위 도달 시 넥스트 버튼 클릭\n",
    "            driver.find_element(By.CSS_SELECTOR, '.btn_next').click()\n",
    "            # continue\n",
    "\n",
    "        \n",
    "        \n",
    "def file_export():\n",
    "    global contents_no\n",
    "    \n",
    "    df = pd.DataFrame({\"제목\":title_list, \"내용\":contents_list}, index=None)\n",
    "\n",
    "    filename = f'{search}여행기사_{goal_cnt}건_{date_format}.csv'\n",
    "    fileaddr = f_dir+'\\\\'+filename\n",
    "    \n",
    "    if not os.path.exists(fileaddr):\n",
    "        df.to_csv(fileaddr, index= False, mode='w', encoding='utf-8-sig')\n",
    "    else:\n",
    "        df.to_csv(fileaddr, index= False, mode='a', encoding='utf-8-sig', header=False)\n",
    "    \n",
    "    print(f'====== 콘텐츠 {contents_no-1}개, {filename} 파일 저장 완료 ======')\n",
    "    \n",
    "    img_no = 0\n",
    "    for src in img_url_list:\n",
    "        # 다운로드  (주소, 파일이름)\n",
    "        img_no += 1\n",
    "        urllib.request.urlretrieve(src, f'{f_dir}\\\\{page_no}_{img_no}.jpg')\n",
    "        \n",
    "    print(f'====== 이미지 {img_no}개 저장 완료 ======')\n",
    "\n",
    "today = time.localtime()\n",
    "print('스크래핑 프로그램 실행')\n",
    "\n",
    "for page_no in range(1, page_cnt+1):    \n",
    "    print(f'====== {page_no} 페이지 스크래핑 시작 ======')\n",
    "    page_work()\n",
    "    print(f'====== {page_no} 페이지 콘텐츠 저장 중 ======')\n",
    "    file_export()\n",
    "    print(f'====== {page_no} 페이지 스크래핑 완료 ======')\n",
    "    if page_no < page_cnt:\n",
    "        next_button = driver.find_element(By.CSS_SELECTOR, f\"a[id='{page_no+1}']\")\n",
    "        driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "        time.sleep(2)\n",
    "                   \n",
    "print('스크래핑 프로그램 종료')\n",
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
